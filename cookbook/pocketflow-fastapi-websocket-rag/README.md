# PocketFlow FastAPI WebSocket Chat

Real-time chat interface with streaming LLM responses using PocketFlow, FastAPI, and WebSocket.

<p align="center">
  <img 
    src="./assets/banner.png" width="800"
  />
</p>
## How It Works

The magic happens through a two-phase pipeline implemented with PocketFlow:

```mermaid
flowchart TD
    %% å·¦ä¾§åˆ— - æ–‡æ¡£å¤„ç†æµç¨‹
    subgraph LeftColumn [æ–‡æ¡£å¤„ç†æµç¨‹]
        direction TB
        A1[ğŸ“„ æ–‡æ¡£å¯¼å…¥] --> A2[æ–‡æ¡£åŠ è½½ä¸è§£æ]
        A2 --> A3[æ–‡æœ¬åˆ†å‰²<br>Text Splitting]
        A3 --> A4[æ–‡æœ¬å‘é‡åŒ–<br>Embedding]
        A4 --> A5[å‘é‡å­˜å‚¨]
    end

    %% ä¸­é—´åˆ— - å‘é‡æ•°æ®åº“
    subgraph MiddleColumn [å‘é‡æ•°æ®åº“]
        direction TB
        DB[(å‘é‡æ•°æ®åº“<br>Vector Store)]
    end
    
    %% å³ä¾§åˆ— - æŸ¥è¯¢ä¸å“åº”æµç¨‹
    subgraph RightColumn [æŸ¥è¯¢ä¸å“åº”æµç¨‹]
        direction TB
        B1[â“ ç”¨æˆ·è¾“å…¥æŸ¥è¯¢] --> B2[æŸ¥è¯¢é¢„å¤„ç†]
        B2 --> B3[æŸ¥è¯¢å‘é‡åŒ–<br>Query Embedding]
        B3 --> B4[ç›¸ä¼¼æ€§æ£€ç´¢<br>Similarity Search]
        B4 --> C1[é‡æ’åº<br>Re-ranking]
        C1 --> C2[é€‰æ‹©æœ€ç›¸å…³ç‰‡æ®µ]
        C2 --> D1[ç»„åˆæŸ¥è¯¢ä¸ä¸Šä¸‹æ–‡]
        D1 --> D2[LLMç”Ÿæˆå›ç­”<br>Large Language Model]
        D2 --> D3[âœ… è¿”å›æœ€ç»ˆç­”æ¡ˆ]
    end
    
    %% è¿æ¥å·¦ä¾§å’Œä¸­é—´åˆ—
    A5 --> DB
    
    %% è¿æ¥ä¸­é—´åˆ—å’Œå³ä¾§åˆ—
    DB --> B4
    
    %% æ ·å¼å®šä¹‰
    classDef docProcess fill:#E3F2FD,stroke:#0D47A1,stroke-width:2px,color:#0D47A1;
    classDef vectorDB fill:#FFEBEE,stroke:#C62828,stroke-width:2px,color:#C62828;
    classDef queryProcess fill:#E8F5E9,stroke:#2E7D32,stroke-width:2px,color:#2E7D32;
    classDef rerankProcess fill:#FFF8E1,stroke:#FF8F00,stroke-width:2px,color:#FF8F00;
    classDef responseProcess fill:#F3E5F5,stroke:#4A148C,stroke-width:2px,color:#4A148C;
    
    %% åº”ç”¨æ ·å¼
    class A1,A2,A3,A4,A5 docProcess;
    class DB vectorDB;
    class B1,B2,B3,B4 queryProcess;
    class C1,C2 rerankProcess;
    class D1,D2,D3 responseProcess;
```
## Features

- **Real-time Streaming**: See AI responses typed out in real-time as the LLM generates them
- **Conversation Memory**: Maintains chat history across messages
- **Modern UI**: Clean, responsive chat interface with gradient design
- **WebSocket Connection**: Persistent connection for instant communication
- **PocketFlow Integration**: Uses PocketFlow `AsyncNode` and `AsyncFlow` for streaming

## How to Run

1. **Set OpenAI API Key:**
   ```bash
   export OPENAI_API_KEY="your-openai-api-key"
   ```

2. **Install Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the Application:**
   ```bash
   python main.py
   ```

4. **Access the Web UI:**
   Open `http://localhost:8000` in your browser.

## Usage

1. **Type Message**: Enter your message in the input field
2. **Send**: Press Enter or click Send button
3. **Watch Streaming**: See the AI response appear in real-time
4. **Continue Chat**: Conversation history is maintained automatically

## Files

- [`main.py`](./main.py): FastAPI application with WebSocket endpoint
- [`nodes.py`](./nodes.py): PocketFlow `StreamingChatNode` definition
- [`flow.py`](./flow.py): PocketFlow `AsyncFlow` for chat processing
- [`utils/stream_llm.py`](./utils/stream_llm.py): OpenAI streaming utility
- [`static/index.html`](./static/index.html): Modern chat interface
- [`requirements.txt`](./requirements.txt): Project dependencies
- [`docs/design.md`](./docs/design.md): System design documentation
- [`README.md`](./README.md): This file 